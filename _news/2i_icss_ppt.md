---
layout: post
title: Presenting at iCSS Seminar Series
date: 2025-10-28
inline: false
related_posts: false
---



<p align="center">
  <img src="/assets/img/icss2.jpeg" alt="iCSS Presentation" style="max-width: 100%; height: auto;" />
</p>

I was delighted to present at the [Institute of Cyber Security for Society (iCSS)](https://research.kent.ac.uk/cyber/) Seminar Series at the [University of Kent](https://www.kent.ac.uk/), sharing highlights from my latest paper, <a href="/assets/pdf/analysing_llm_risks_paper.pdf" target="_blank" rel="noopener noreferrer">Analysing Safety Risks in LLMs Fine‑Tuned with Pseudo‑Malicious Cyber Security Data</a>.

For a non‑technical audience: I explored how AI language models (LLMs) used in cyber security can unintentionally produce unsafe or risky advice after being trained on security‑related content. We systematically checked where things can go wrong and proposed a practical way to “coach” these models to include safety guardrails in their answers—so they stay useful for professionals while reducing the chance of harmful outputs.

<p align="center">
  <img src="/assets/img/icss3.jpeg" alt="iCSS Presentation" style="max-width: 100%; height: auto;" />
</p>

<p align="center">
  <img src="/assets/img/icss4.jpeg" alt="iCSS Presentation" style="max-width: 100%; height: auto;" />
</p>
